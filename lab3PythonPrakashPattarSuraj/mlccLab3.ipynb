{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLCC - Laboratory 3 - Dimensionality reduction and feature selection\n",
    "In this laboratory we will address the problem of data analysis with a reference to a classification problem. \n",
    "Follow the instructions below.\n",
    "\n",
    "Import all the functions from the file \"lab3ImpFunction.py\" by: <br>\n",
    "from lab3ImpFunction import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Warm up - data generation\n",
    "\n",
    "You will generate a training and a test set of D-dimensional points (N points for each class), with N=100 D=30. Only two of those dimensions will be meaningful, the other ones will be an irrelevant noise. <br>\n",
    "\n",
    "N=100 \n",
    "\n",
    "D=30 \n",
    "\n",
    "   * **1.A** *For each point, the first two variables will be generated by MixGauss, extracted from two gaussian distributions with centroids (1, 1) and (-1,-1) and standard deviation 0.7 (the first one with Y=1, the second with Y=-1)* \n",
    "   \n",
    "   Xtr, Ytr = MixGauss(np.matrix([[1,-1],[1,1]]),np.array([[0.7], [0.7]]),N); \n",
    "   \n",
    "   Ytr[Ytr==2]= -1; \n",
    "   \n",
    "   Xts, Yts = MixGauss(np.matrix([[1,-1],[1,1]]),np.array([[0.7],[0.7]]),N); \n",
    "   \n",
    "   Yts[Yts==2] = -1; \n",
    "   \n",
    "   \n",
    "   * **2.A** *You may want to plot the relevant variables of the data* \n",
    "   \n",
    "   plt.scatter(Xtr[:,0], Xtr[:,1], s=15, c=Ytr, alpha=0.5) \n",
    "   \n",
    "   plt.scatter(Xts[:,0], Xts[:,1], s=30, c=Yts, alpha=0.8) \n",
    "   \n",
    "   *Use plt.show() to display the graph*\n",
    "   \n",
    "   \n",
    "   * **3.A** *The remaining variables will be generated as gaussian noise* \n",
    "   \n",
    "   sigma_noise = 0.01; \n",
    "\n",
    "   Xtr_noise = sigma_noise * np.random.randn(2*N, D-2); \n",
    "   \n",
    "   Xts_noise = sigma_noise * np.random.randn(2*N, D-2); \n",
    "   \n",
    "   *To compose the final data matrix, run:*\n",
    "   \n",
    "   Xtr = np.concatenate((Xtr, Xtr_noise), axis=1)\n",
    "   \n",
    "   Xts = np.concatenate((Xts, Xts_noise), axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Principal Component Analysis\n",
    "\n",
    "   * **2.A** *Compute the data principal components (see help PCA)*\n",
    "   \n",
    "    \n",
    "   * **2.B** *Plot the first two components of X_proj using the following line*\n",
    "    \n",
    "   `plt.scatter(X_proj[:,1], X_proj[:,2], s=20, c=Ytr, alpha=0.5)`\n",
    "   \n",
    "    \n",
    "   * **2.C** *Try now with the first 3 components, by using*\n",
    "    \n",
    "   `fig = pyplot.figure()`\n",
    "    \n",
    "   `ax = Axes3D(fig)`\n",
    "\n",
    "   `x = X_proj[:,0].real`\n",
    "    \n",
    "   `y = X_proj[:,1].real`\n",
    "    \n",
    "   `z = X_proj[:,2].real`\n",
    "\n",
    "   `ax.scatter(x, y, z, c=Ytr, marker='o')`\n",
    "\n",
    "   `ax.set_xlabel('X Label')`\n",
    "    \n",
    "   `ax.set_ylabel('Y Label')`\n",
    "    \n",
    "   `ax.set_zlabel('Z Label')`\n",
    "\n",
    "   `pyplot.show()`\n",
    "    \n",
    "   *Reason on the meaning of the results you are obtaining*\n",
    "   \n",
    "    \n",
    "   * **2.D** *Display the sqrt of the first 10 eigenvalues:*\n",
    "    \n",
    "   `print(np.sqrt(d[:10]))` \n",
    "    \n",
    "   *Plot the coefficients (eigenvector) associated with the largest eigenvalue:*\n",
    "    \n",
    "   `plt.scatter(list(range(D)), abs(V[:,0]))`\n",
    "    \n",
    "   `plt.show()`\n",
    "   \n",
    "   \n",
    "   * **2.E** *Repeat the above steps with dataset generated using different sigma_noise (0, 0.01, 0.1, 0.5, 0.7, 1, 1.2, 1.4, 1.6, 2). To what extent data visualization by PCA is affected by the noise?*\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable selection\n",
    "\n",
    "   * **3.A** *Use the data generated in section 1. Standardize the data matrix, so that each column has mean 0 and standard deviation 1*\n",
    "   \n",
    "   ```m = np.mean(Xtr, axis=0);``` *(see \"help (np.mean)\", it computes the mean for each column)*\n",
    "   \n",
    "   `s = np.std(Xtr, axis=0)`\n",
    "\n",
    "   ```for i in range(2*N):\n",
    "       Xtr[i, :] = Xtr[i, :] - m\n",
    "       Xts[i, :] = Xts[i, :] - m```\n",
    "    \n",
    "   ```for i in range(2*N):\n",
    "       Xtr[i, :] = Xtr[i, :] / s\n",
    "       Xts[i, :] = Xts[i, :] / s```\n",
    "       \n",
    "   *Do the same for Xts, by using m and s computed on Xtr.*\n",
    "   \n",
    "   \n",
    "   \n",
    "   * **3.B** *Use the orthogonal matching pursuit algorithm (type 'help OMatchingPursuit')*\n",
    "   \n",
    "   `Ypred = np.sign(Xts.dot(w))`\n",
    "\n",
    "   `err = calcErr(Yts, Ypred);`\n",
    "   \n",
    "   *and plot the coefficients w with* \n",
    "   \n",
    "   `plt.scatter(list(range(D)), abs(w))`\n",
    "   \n",
    "   *How the error changes with the number of iterations of the method?*\n",
    "   \n",
    "   \n",
    "   * **3.D** *By using the method holdoutCVOMP find the best number of iterations with `intIter = list(range(1,D))` (and, for instance, `perc=0.75` `nrip = 20`).*\n",
    "   \n",
    "   *Moreover, plot the training and validation error with the following lines:*\n",
    "   \n",
    "   `plt.plot(intIter, Tm, 'r+');`\n",
    "\n",
    "   `plt.plot(intIter, Vm, 'b+');`\n",
    "   \n",
    "   *What is the behavior of the training and the validation errors with respect to the number of iterations?*\n",
    "   \n",
    "   \n",
    "   * **3.E** *Try to increase the number of relevant variables `d = 3,5,..` (and the corresponding standard deviation of the Gaussians) around the centroids*\n",
    "   \n",
    "   `np.ones((d,1));` *# vector of all 1s*\n",
    "   \n",
    "   `-np.ones(d,1);` *# vector of all -1s*\n",
    "   \n",
    "   *and see how this change is reflected in the cross-validation.*\n",
    "   \n",
    "   \n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If you have time - More experiments\n",
    "\n",
    "   * **4.A** *Analyse the results you obtain on sections 2 and 3 once you choose*\n",
    "   \n",
    "      * `N >> D`\n",
    "      * `N ~ D`\n",
    "      * `N << D`\n",
    "      \n",
    "      *and evaluate the benefits of the two different analysis*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
